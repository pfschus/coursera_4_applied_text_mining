{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 notes\n",
    "\n",
    "Semantic Text Similarity\n",
    "\n",
    "- Useful when grouping words by their meaning  \n",
    "- Useful as a building block in national language understanding tasks  \n",
    "    - Textual entailment  \n",
    "    - Paraphrasing\n",
    "\n",
    "# WordNet\n",
    "\n",
    "- a symantic dictionary, interlinked by semantic relations  \n",
    "- Includes rich linguistic information- part of speech, word sense, synonyms, etc  \n",
    "- Machine-readable, freely available  \n",
    "- Organizes information in a hierarchy. Calculate path similarity: Find the shortest path between two concepts, similarity is inversely related to path distance  \n",
    "    - Lowest common subsumer (LCS): the closest ancestor to two separate concepts  \n",
    "    - Lin similarity: Uses LCS to measure similarity\n",
    "    \n",
    "How to do all this in Python?  \n",
    "- WordNet is easily imported through NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find path similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sense of the word- n.01 = first noun meaning\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk  = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(elk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Lin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat') # Import brown clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623778273893673"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.lin_similarity(elk, brown_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726998936065773"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.lin_similarity(horse,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations and distributional similarity\n",
    "\n",
    "Words that frequently appear in similar contexts are more likely to be semantically related. Options:  \n",
    "- Look for words within a window before or after each other. \n",
    "- Parts of speech that occur with the target word  \n",
    "- Syntatic relation  \n",
    "- Words in the same sentence, same document, etc...\n",
    "\n",
    "You can calculate the strength of the association between words\n",
    "\n",
    "Pointwise Mutual Information = Chance of seeing the words together / (chance of seeing first word * chance of seeing second word)\n",
    "\n",
    "How to do it in Python:\n",
    "\n",
    "    import nltk\n",
    "    from nltk.collocations import *\n",
    "    bigrams_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    finder = BigramCollocationFinder.from_words(text)\n",
    "    finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "You can also use `finder` for frequency filtering  \n",
    "\n",
    "    finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "A generative model used extensively for modeling large text corpa\n",
    "\n",
    "    import genism  \n",
    "    from genism import corpora, models  \n",
    "    dictionary = corpora.Dictionary(doc_set)  \n",
    "    corpus = [dictionary.doc2bow(doc) for doc in doc_set]   \n",
    "    ldamodel = genism.models.ldamodel.LdaModel(corpus, num_topics = 4, id2word = dictionary, passes=50)  \n",
    "    print(ldamodel.print_topics(num_topics=4, num_words=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
